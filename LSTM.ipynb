{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae7bb3-4d48-47d0-abd4-2b41d3ac5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa2ec6-10b4-4c3c-9d3f-3994e23ab804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af65ba-0a5b-47b9-be41-15ae757b17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = Path('dataset')\n",
    "EMOTIONS = {\n",
    "    'Neutral': 0,\n",
    "    'Happy': 1,\n",
    "    'Sad': 2,\n",
    "    'Anger': 3,\n",
    "    'Fear': 4,\n",
    "    'Disgust': 5,\n",
    "    'Surprise': 6\n",
    "}\n",
    "TRAIN_SPLIT = 0.8\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "LAYER_COUNT = 4\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "GRADIENT_CLIPPING_MAX_NORM = 0.5\n",
    "MAX_EPOCHS = 3000\n",
    "EARLY_STOPPING_PATIENCE = 200\n",
    "PRINT_FREQUENCY_EPOCHS = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ebd78-32e5-4615-8ec0-a9f4d3f64d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeedDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.features = []\n",
    "        for subject_index in range(1, 21):\n",
    "            subject_features = sio.loadmat(str(DATASET_PATH / 'EEG_features'\n",
    "                                               / f'{subject_index}.mat'))\n",
    "            for video_index in range(1, 81):\n",
    "                de_features = subject_features[f'de_LDS_{video_index}']\n",
    "                # Flatten the frequency band and EEG channel dimensions.\n",
    "                de_features = de_features.reshape(de_features.shape[0], -1)\n",
    "                # Shape: (sequence length, input size (5 * 62))\n",
    "                self.features.append(de_features)\n",
    "        labels = pd.read_excel(\n",
    "            DATASET_PATH / 'emotion_label_and_stimuli_order.xlsx', header=None,\n",
    "            usecols='B:U', skiprows=lambda row_index: row_index % 2 == 0\n",
    "        )\n",
    "        labels = labels.values.flatten().tolist()\n",
    "        labels = [EMOTIONS[label] for label in labels]\n",
    "        labels = labels * 20\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[np.ndarray, int]:\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a30b6-68bd-4e7c-b82a-a4232d346ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loaders() -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    dataset = SeedDataset()\n",
    "    row_count = len(dataset)\n",
    "    train_indices = list(range(0, int(TRAIN_SPLIT * row_count)))\n",
    "    validation_indices = list(range(\n",
    "        int(TRAIN_SPLIT * row_count),\n",
    "        int((TRAIN_SPLIT + VALIDATION_SPLIT) * row_count)))\n",
    "    test_indices = list(range(\n",
    "        int((TRAIN_SPLIT + VALIDATION_SPLIT) * row_count), row_count))\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    validation_dataset = Subset(dataset, validation_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    def collate_fn(batch: list[tuple[np.ndarray, int]]) -> tuple:\n",
    "        sequences = [torch.tensor(sequences_and_label[0], dtype=torch.float)\n",
    "                     for sequences_and_label in batch]\n",
    "        labels = torch.tensor([sequences_and_label[1]\n",
    "                               for sequences_and_label in batch],\n",
    "                              dtype=torch.long)\n",
    "        sequence_lengths = torch.tensor([sequence.shape[0]\n",
    "                                         for sequence in sequences],\n",
    "                                        dtype=torch.long)\n",
    "        # Shape: (batch size, max sequence length, input size)\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "        return padded_sequences, sequence_lengths, labels\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE,\n",
    "                                   shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc350bdb-64a6-4c36-ac58-a9ac99e827eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LstmClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LstmClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=5 * 62, hidden_size=HIDDEN_SIZE,\n",
    "                            num_layers=LAYER_COUNT, batch_first=True)\n",
    "        self.linear = nn.Linear(HIDDEN_SIZE, len(EMOTIONS))\n",
    "\n",
    "    def forward(self, batch: torch.Tensor,\n",
    "                sequence_lengths: torch.Tensor) -> torch.Tensor:\n",
    "        packed_batch = pack_padded_sequence(batch, sequence_lengths.cpu(),\n",
    "                                            batch_first=True,\n",
    "                                            enforce_sorted=False)\n",
    "        _, (final_hidden_states, _) = self.lstm(packed_batch)\n",
    "        output = self.linear(final_hidden_states[-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4035aa9-d5f5-411a-b1d1-0cd95e3da4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model: nn.Module, data_loader: DataLoader,\n",
    "                optimizer: optim.Optimizer, criterion: nn.Module,\n",
    "                device: torch.device) -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        sequences, sequence_lengths, labels = batch\n",
    "        sequences = sequences.to(device)\n",
    "        sequence_lengths = sequence_lengths.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(sequences, sequence_lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                       max_norm=GRADIENT_CLIPPING_MAX_NORM)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * labels.shape[0]\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        total_count += labels.shape[0]\n",
    "        correct_count += (predictions == labels).sum().item()\n",
    "    loss = total_loss / total_count\n",
    "    accuracy = correct_count / total_count\n",
    "    return loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133cfb3a-04a1-4ebd-b93d-6c5048350b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model: nn.Module, data_loader: DataLoader, criterion: nn.Module,\n",
    "             device: torch.device) -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            sequences, sequence_lengths, labels = batch\n",
    "            sequences = sequences.to(device)\n",
    "            sequence_lengths = sequence_lengths.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(sequences, sequence_lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * labels.shape[0]\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            total_count += labels.shape[0]\n",
    "            correct_count += (predictions == labels).sum().item()\n",
    "    loss = total_loss / total_count\n",
    "    accuracy = correct_count / total_count\n",
    "    return loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6d29d-e744-4cd2-9df5-a8d671d97b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train() -> tuple[list[float], list[float], list[float], list[float]]:\n",
    "    train_loader, validation_loader, test_loader = get_data_loaders()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}.')\n",
    "    model = LstmClassifier().to(device)\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    best_validation_loss = float('inf')\n",
    "    best_model = None\n",
    "    epochs_without_improvement = 0\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        train_loss, train_accuracy = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device)\n",
    "        validation_loss, validation_accuracy = evaluate(\n",
    "            model, validation_loader, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        if epoch == 0 or (epoch + 1) % PRINT_FREQUENCY_EPOCHS == 0:\n",
    "            print(f'Epoch {epoch + 1}: '\n",
    "                  f'Train loss: {train_loss:.4f}, '\n",
    "                  f'Train accuracy: {train_accuracy:.4f}, '\n",
    "                  f'Validation loss: {validation_loss:.4f}, '\n",
    "                  f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            best_model = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f'Stopping because validation loss did not improve for '\n",
    "                  f'{EARLY_STOPPING_PATIENCE} epochs. Best validation loss: '\n",
    "                  f'{best_validation_loss:.4f}')\n",
    "            break\n",
    "    model.load_state_dict(best_model)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')\n",
    "    torch.save(model.state_dict(), 'lstm_classifier.pt')\n",
    "    print('Model saved as `lstm_classifier.pt`.')\n",
    "    return (train_losses, train_accuracies, validation_losses,\n",
    "            validation_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d9b239-3e6d-4bbc-9f54-2af6188a77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_curves(train_losses: list[float],\n",
    "                         train_accuracies: list[float],\n",
    "                         validation_losses: list[float],\n",
    "                         validation_accuracies: list[float]):\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_losses, label='Train loss')\n",
    "    plt.plot(epochs, validation_losses, label='Validation loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss.png')\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_accuracies, label='Train accuracy')\n",
    "    plt.plot(epochs, validation_accuracies, label='Validation accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('accuracy.png')\n",
    "    print('Loss and accuracy curves saved as `loss.png` and `accuracy.png`.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812db27c-3747-446b-8d37-7dad5191a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    (train_losses, train_accuracies, validation_losses,\n",
    "     validation_accuracies) = train()\n",
    "    plot_training_curves(train_losses, train_accuracies, validation_losses,\n",
    "                         validation_accuracies)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
