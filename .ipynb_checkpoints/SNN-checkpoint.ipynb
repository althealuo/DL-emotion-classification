{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae7bb3-4d48-47d0-abd4-2b41d3ac5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa2ec6-10b4-4c3c-9d3f-3994e23ab804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349e669-e11d-433f-a3a7-9d32a3e7695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import snntorch.spikeplot as splt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d5850-4dd8-460d-ae49-16a735df11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af65ba-0a5b-47b9-be41-15ae757b17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = Path('dataset')\n",
    "EMOTIONS = {\n",
    "    'Neutral': 0,\n",
    "    'Happy': 1,\n",
    "    'Sad': 2,\n",
    "    'Anger': 3,\n",
    "    'Fear': 4,\n",
    "    'Disgust': 5,\n",
    "    'Surprise': 6\n",
    "}\n",
    "TRAIN_SPLIT = 0.8\n",
    "VALIDATION_SPLIT = 0\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "LAYER_COUNT = 4\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "GRADIENT_CLIPPING_MAX_NORM = 0.5\n",
    "MAX_EPOCHS = 3000\n",
    "EARLY_STOPPING_PATIENCE = 200\n",
    "PRINT_FREQUENCY_EPOCHS = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ebd78-32e5-4615-8ec0-a9f4d3f64d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeedDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.features = []\n",
    "        for subject_index in range(1, 21):\n",
    "            subject_features = sio.loadmat(str(DATASET_PATH / 'EEG_features'\n",
    "                                               / f'{subject_index}.mat'))\n",
    "            for video_index in range(1, 81):\n",
    "                de_features = subject_features[f'de_LDS_{video_index}']\n",
    "                # Flatten the frequency band and EEG channel dimensions.\n",
    "                de_features = de_features.reshape(de_features.shape[0], -1)\n",
    "                # Shape: (sequence length, input size (5 * 62))\n",
    "                self.features.append(de_features)\n",
    "        labels = pd.read_excel(\n",
    "            DATASET_PATH / 'emotion_label_and_stimuli_order.xlsx', header=None,\n",
    "            usecols='B:U', skiprows=lambda row_index: row_index % 2 == 0\n",
    "        )\n",
    "        labels = labels.values.flatten().tolist()\n",
    "        labels = [EMOTIONS[label] for label in labels]\n",
    "        labels = labels * 20\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[np.ndarray, int]:\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a30b6-68bd-4e7c-b82a-a4232d346ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loaders() -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    dataset = SeedDataset()\n",
    "    row_count = len(dataset)\n",
    "    train_indices = list(range(0, int(TRAIN_SPLIT * row_count)))\n",
    "    validation_indices = list(range(\n",
    "        int(TRAIN_SPLIT * row_count),\n",
    "        int((TRAIN_SPLIT + VALIDATION_SPLIT) * row_count)))\n",
    "    test_indices = list(range(\n",
    "        int((TRAIN_SPLIT + VALIDATION_SPLIT) * row_count), row_count))\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    validation_dataset = Subset(dataset, validation_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "    def collate_fn(batch: list[tuple[np.ndarray, int]]) -> tuple:\n",
    "        sequences = [torch.tensor(sequences_and_label[0], dtype=torch.float)\n",
    "                     for sequences_and_label in batch]\n",
    "        labels = torch.tensor([sequences_and_label[1]\n",
    "                               for sequences_and_label in batch],\n",
    "                              dtype=torch.long)\n",
    "        sequence_lengths = torch.tensor([sequence.shape[0]\n",
    "                                         for sequence in sequences],\n",
    "                                        dtype=torch.long)\n",
    "        # Shape: (batch size, max sequence length, input size)\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "        return padded_sequences, sequence_lengths, labels\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE,\n",
    "                                   shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12696027-e34f-47b8-bf11-1f355e68106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader, test_loader = get_data_loaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076f47d-b3d5-4102-8672-86e33c03cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode 3: each input feature is used as the probability a spike occurs at any given time step\n",
    "def gen_spike_data_bernoulli(x, T=50):\n",
    "    \"\"\"\n",
    "    Encodes input data into spike trains using a Bernoulli process.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input data tensor of shape [batch_size, num_features].\n",
    "        T (int): Number of time steps for the spike train.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Spike train tensor of shape [T, batch_size, num_features].\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for t in range(T):\n",
    "        # Generate spikes based on Bernoulli trials for each feature\n",
    "        encoded = torch.bernoulli(x)\n",
    "        res.append(encoded)\n",
    "    return torch.stack(res)  # Shape: [T, batch_size, num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adebede-6bea-4c33-88c1-113d83492b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the emotion data to [0, 1]\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "# Normalize to [0, 1] for Bernoulli trials\n",
    "X_train_tensor = (X_train_tensor - X_train_tensor.min()) / (X_train_tensor.max() - X_train_tensor.min())\n",
    "X_test_tensor = (X_test_tensor - X_test_tensor.min()) / (X_test_tensor.max() - X_test_tensor.min())\n",
    "\n",
    "# Generate spike data\n",
    "T = 50  # Number of time steps\n",
    "spike_train_train = gen_spike_data_bernoulli(X_train_tensor, T)\n",
    "spike_train_test = gen_spike_data_bernoulli(X_test_tensor, T)\n",
    "\n",
    "print(\"Spike train shape (train):\", spike_train_train.shape)  # Should be [T, batch_size, num_features]\n",
    "print(\"Spike train shape (test):\", spike_train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2737c-5195-4a91-b9c6-48e0ea27f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakySurrogate(nn.Module):\n",
    "    def __init__(self, beta, z=1, threshold=1.0):\n",
    "        super(LeakySurrogate, self).__init__()\n",
    "\n",
    "        # initialize decay rate beta and threshold\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "        self.spike_op = self.SpikeOperator.apply\n",
    "        self.z = z\n",
    "        self.mem = None\n",
    "\n",
    "    # the forward function is called each time we call Leaky\n",
    "    def forward(self, input_):\n",
    "        spk = self.spike_op(self.mem - self.threshold, self.z)  # call the Heaviside function\n",
    "        reset = (spk * self.threshold).detach() # removes spike_op gradient from reset\n",
    "        self.mem = self.beta * self.mem + input_ - reset\n",
    "        return spk\n",
    "\n",
    "    # forward pass: Heaviside function\n",
    "    @staticmethod\n",
    "    class SpikeOperator(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, z):\n",
    "            \"\"\"\n",
    "            In the forward pass we compute a step function of the input Tensor\n",
    "            and return it. ctx is a context object that we use to stash information which\n",
    "            we need to later backpropagate our error signals. To achieve this we use the\n",
    "            ctx.save_for_backward method.\n",
    "            \"\"\"\n",
    "            ctx.save_for_backward(input)\n",
    "            ctx.z = z\n",
    "            spk = torch.zeros_like(input)\n",
    "            spk[input > 0] = 1.0\n",
    "            return spk\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            \"\"\"\n",
    "            In the backward pass we receive a Tensor we need to compute the\n",
    "            surrogate gradient of the loss with respect to the input.\n",
    "            Here we use the fast Sigmoid function with z = 1.\n",
    "            \"\"\"\n",
    "            input, = ctx.saved_tensors\n",
    "            z = ctx.z\n",
    "            grad_input = grad_output.clone()\n",
    "            # TODO: add your implementation here.\n",
    "            grad = (\n",
    "                grad_input\n",
    "                * z * torch.exp(-z * input)\n",
    "                / (torch.exp(-z * input) + 1) ** 2\n",
    "            )\n",
    "            return grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493079b-c0c4-4cef-b7af-4c29c3c44fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, T, beta=0.8, z=1, threshold=1.0):\n",
    "        super(SNN, self).__init__()\n",
    "        self.T = T\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 1st fully-connected layer\n",
    "        self.fc1 = nn.Linear(1500, 10)\n",
    "        self.lif1 = LeakySurrogate(beta=beta, z=z, threshold=threshold)\n",
    "        # 2nd fully-connected layer\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        # output layer neurons, whose firing rate will be served as the final prediction\n",
    "        self.lif2 = LeakySurrogate(beta=beta, z=z, threshold=threshold)\n",
    "\n",
    "    def init_mem(self, batch_size, feature_num):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(batch_size, feature_num)).to(device)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, input_):\n",
    "        self.lif1.mem = self.init_mem(input_.shape[1], 10)\n",
    "        self.lif2.mem = self.init_mem(input_.shape[1], 10)\n",
    "        \n",
    "        output_spikes = 0\n",
    "        for t in range(self.T):\n",
    "            x = input_[t]\n",
    "            x = self.flatten(x)\n",
    "            x = self.fc1(x)\n",
    "            spk1 = self.lif1(x)\n",
    "            x = self.fc2(spk1)\n",
    "            spk2 = self.lif2(x)\n",
    "            output_spikes = output_spikes + spk2\n",
    "\n",
    "        return output_spikes / self.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
